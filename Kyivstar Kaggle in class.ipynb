{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle (Kyivstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from numpy.random import RandomState\n",
    "import xgboost as xgb\n",
    "import copy \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import cross_validation as cv\n",
    " # Feature Importance\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import ExtraTreesClassifier#plt.hist(data_train[['VOICE_SCH_MORN_6M']])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(r'A:\\KAGGLE\\Kaggle_Kyivstar\\Xtrain.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8532, 42)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### build a single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data_train[['IS_RETIREE']], data_train[['REFILLED_3M']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### count and investigate values of SUPPORT_3G as it consists of only NAs and 1es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is retiree and supports 3G:  192\n",
      "supports 3G and is not retiree:  2647.0\n"
     ]
    }
   ],
   "source": [
    "print('is retiree and supports 3G: ',sum(data_train['IS_RETIREE'][data_train['SUPPORT_3G']==1]))\n",
    "print('supports 3G and is not retiree: ',data_train['SUPPORT_3G'][data_train['IS_RETIREE']==0].sum(skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(data_train['HANDSET_TYPE'].unique().shape)\n",
    "print(data_train['AVG_CHARGE_3M_GROUP'].unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### create new categorical and binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 49\n"
     ]
    }
   ],
   "source": [
    "category = list(set(data_train['HANDSET_VENDOR_CVAL']))\n",
    "category\n",
    "data_train['HANDSET_VENDOR_CVAL_ix'] = [category.index(i) for i in data_train['HANDSET_VENDOR_CVAL']]\n",
    "\n",
    "category = list(set(data_train['HANDSET_TYPE']))\n",
    "category\n",
    "data_train['HANDSET_TYPE_ix'] = [category.index(i) for i in data_train['HANDSET_TYPE']]\n",
    "data_train['AVG_CHARGE_3M_GROUP'] = data_train['AVG_CHARGE_3M_GROUP']-1\n",
    "#data_train['HANDSET_TYPE_ix_grouped'] = [[category.index(i),1][category.index(i) in [2,3,4,6]] for i in data_train['HANDSET_TYPE']]\n",
    "data_train = pd.concat([data_train, pd.get_dummies(data_train['AVG_CHARGE_3M_GROUP'],prefix='gr')], axis=1)\n",
    "data_train = pd.concat([data_train, pd.get_dummies(data_train['HANDSET_TYPE_ix'],prefix='handset')], axis=1)\n",
    "\n",
    "## Correlation:\n",
    "dd = data_train.corr()\n",
    "## Features without dummies\n",
    "all_features1 = [i for i in list(data_train.columns.values) if i not in ['SUBS_ID','HANDSET_VENDOR_CVAL','HANDSET_TYPE', \n",
    "    'gr_1','gr_2','gr_3','gr_4','gr_0','handset_0','handset_1','handset_2','handset_3','handset_4','handset_5','handset_6']]\n",
    "## Features with dummies                                                                    \n",
    "all_features2 = [i for i in list(data_train.columns.values) if i not in ['SUBS_ID','HANDSET_VENDOR_CVAL','HANDSET_TYPE', \n",
    "    'AVG_CHARGE_3M_GROUP','HANDSET_TYPE_ix','gr_0','handset_0']] \n",
    "\n",
    "print(len(all_features1), len(all_features2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_OUT_CNT_3M', 'VOICE_CNT_WRK_DAY_6M']\n",
      "['DATA_MB_3M', 'DATA_CNT_EV_6M']\n",
      "['DATA_MB_3M', 'DATA_CNT_WRK_AFT_EV_6M']\n",
      "['DATA_MB_3M', 'DATA_CNT_WRK_DAY_6M']\n",
      "['OUT_ROUM_MIN_3M', 'OUT_ROUM_CNT_3M']\n",
      "['OUT_ROUM_MIN_3M', 'IN_ROUM_CNT_3M']\n",
      "['IN_ROUM_MIN_3M', 'IN_ROUM_CNT_3M']\n",
      "['OUT_ROUM_CNT_3M', 'IN_ROUM_CNT_3M']\n",
      "['DATA_CNT_EV_6M', 'DATA_CNT_WRK_AFT_EV_6M']\n",
      "['DATA_CNT_SCH_MORN_6M', 'DATA_CNT_WRK_DAY_6M']\n",
      "['SMS_CNT_EV_6M', 'SMS_CNT_NGHT_6M']\n",
      "['SMS_CNT_EV_6M', 'SMS_CNT_WRK_AFT_EV_6M']\n",
      "['SMS_CNT_EV_6M', 'SMS_CNT_WRK_DAY_6M']\n",
      "['SMS_CNT_NGHT_6M', 'SMS_CNT_WRK_AFT_EV_6M']\n",
      "['SMS_CNT_NGHT_6M', 'SMS_CNT_WRK_DAY_6M']\n",
      "['SMS_CNT_WRK_AFT_EV_6M', 'SMS_CNT_WRK_DAY_6M']\n",
      "['VOICE_CNT_EV_6M', 'VOICE_WRK_AFT_EV_6M']\n",
      "['VOICE_SCH_MORN_6M', 'VOICE_CNT_WRK_DAY_6M']\n",
      "['VOICE_DUR_EV_6M', 'VOICE_DUR_WRK_AFT_EV_6M']\n"
     ]
    }
   ],
   "source": [
    "## Show all pairs of highly correlated classes:\n",
    "s = list()\n",
    "for i in range(len(all_features1)):\n",
    "    for j in range(i,len(all_features1)):\n",
    "        f1 = all_features1[i]\n",
    "        f2 = all_features1[j]\n",
    "        if dd[f1][f2]>0.9 and f1!=f2:\n",
    "            s.append([f1,f2])\n",
    "for pair in s:\n",
    "    print(pair)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG_CHARGE_3M_GROUP  mean: 2.0042194092827006  median: 2.0  max: 4  min: 0  count_non_na: 8532  count: 5\n",
      "V_OUT_INT_DUR_3M  mean: 0.002877974446137616  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8531  count: 133\n",
      "V_IN_INT_DUR_3M  mean: 0.0015301840346969807  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8531  count: 81\n",
      "V_OUT_INT_CNT_3M  mean: 0.004047122259992959  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8531  count: 123\n",
      "V_IN_INT_CNT_3M  mean: 0.006025905521040979  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8531  count: 142\n",
      "V_OUT_DUR_3M  mean: 0.04256417770484064  median: 0.022000000000000002  max: 1.0  min: 0.0  count_non_na: 8531  count: 322\n",
      "V_IN_DUR_3M  mean: 0.056199976556088896  median: 0.037000000000000005  max: 1.0  min: 0.0  count_non_na: 8531  count: 350\n",
      "V_OUT_CNT_3M  mean: 0.08585464775524472  median: 0.055  max: 1.0  min: 0.0  count_non_na: 8531  count: 466\n",
      "V_IN_CNT_3M  mean: 0.07334415660532131  median: 0.052000000000000005  max: 1.0  min: 0.0  count_non_na: 8531  count: 388\n",
      "REFILLED_3M  mean: 0.012200797092955441  median: 0.008  max: 1.0  min: 0.0  count_non_na: 8531  count: 132\n",
      "DATA_MB_3M  mean: 0.00507771656312274  median: 0.0  max: 0.987  min: 0.0  count_non_na: 8531  count: 147\n",
      "CNT_SMS_3M  mean: 0.0024524674715742803  median: 0.0  max: 0.675  min: 0.0  count_non_na: 8531  count: 107\n",
      "CNT_MMS_3M  mean: 0.0023228226468174754  median: 0.0  max: 0.908  min: 0.0  count_non_na: 8531  count: 45\n",
      "OUT_ROUM_MIN_3M  mean: 0.00021451178056499818  median: 0.0  max: 0.879  min: 0.0  count_non_na: 8531  count: 25\n",
      "IN_ROUM_MIN_3M  mean: 0.00036806939397491487  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8531  count: 35\n",
      "OUT_ROUM_CNT_3M  mean: 0.00015273707654436745  median: 0.0  max: 0.44  min: 0.0  count_non_na: 8531  count: 11\n",
      "IN_ROUM_CNT_3M  mean: 0.00034626655726175147  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8531  count: 14\n",
      "SUPPORT_3G  mean: 1.0  median: 1.0  max: 1.0  min: 1.0  count_non_na: 2839  count: 2\n",
      "DATA_CNT_EV_6M  mean: 0.007543674522218326  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8529  count: 191\n",
      "DATA_CNT_NGHT_6M  mean: 0.0038378829419478105  median: 0.0  max: 0.919  min: 0.0  count_non_na: 8389  count: 126\n",
      "DATA_CNT_SCH_MORN_6M  mean: 0.005849835757860156  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8524  count: 165\n",
      "DATA_CNT_WRK_AFT_EV_6M  mean: 0.007402673545966222  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8528  count: 191\n",
      "DATA_CNT_WRK_DAY_6M  mean: 0.005371894045944679  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8532  count: 154\n",
      "SMS_CNT_EV_6M  mean: 0.001522218313987535  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8529  count: 67\n",
      "SMS_CNT_NGHT_6M  mean: 0.0014120872571224035  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8389  count: 73\n",
      "SMS_CNT_SCH_MORN_6M  mean: 0.004961168465509192  median: 0.002  max: 1.0  min: 0.0  count_non_na: 8524  count: 109\n",
      "SMS_CNT_WRK_AFT_EV_6M  mean: 0.0016266416510318522  median: 0.0  max: 1.0  min: 0.0  count_non_na: 8528  count: 69\n",
      "SMS_CNT_WRK_DAY_6M  mean: 0.00263080168776385  median: 0.001  max: 0.95  min: 0.0  count_non_na: 8532  count: 81\n",
      "VOICE_CNT_EV_6M  mean: 0.09417282213623994  median: 0.067  max: 1.0  min: 0.0  count_non_na: 8529  count: 470\n",
      "VOICE_CNT_NGHT_6M  mean: 0.02765836214089837  median: 0.013000000000000001  max: 1.0  min: 0.0  count_non_na: 8389  count: 265\n",
      "VOICE_SCH_MORN_6M  mean: 0.04944110746128503  median: 0.034  max: 1.0  min: 0.0  count_non_na: 8524  count: 295\n",
      "VOICE_WRK_AFT_EV_6M  mean: 0.07905018761726014  median: 0.055  max: 1.0  min: 0.0  count_non_na: 8528  count: 415\n",
      "VOICE_CNT_WRK_DAY_6M  mean: 0.055188818565400304  median: 0.039  max: 1.0  min: 0.0  count_non_na: 8532  count: 318\n",
      "VOICE_DUR_EV_6M  mean: 0.09697854379176903  median: 0.079  max: 1.0  min: 0.0  count_non_na: 8529  count: 407\n",
      "VOICE_DUR_NGHT_6M  mean: 0.07351591369650665  median: 0.043  max: 0.921  min: 0.0  count_non_na: 8389  count: 475\n",
      "VOICE_DUR_SCH_MORN_6M  mean: 0.0933303613327079  median: 0.075  max: 1.0  min: 0.0  count_non_na: 8524  count: 420\n",
      "VOICE_DUR_WRK_AFT_EV_6M  mean: 0.10525926360225137  median: 0.086  max: 0.936  min: 0.0  count_non_na: 8528  count: 440\n",
      "VOICE_DUR_WRK_DAY_6M  mean: 0.09526347866854211  median: 0.079  max: 1.0  min: 0.0  count_non_na: 8532  count: 403\n",
      "IS_RETIREE  mean: 0.10806375996249414  median: 0.0  max: 1  min: 0  count_non_na: 8532  count: 2\n",
      "HANDSET_VENDOR_CVAL_ix  mean: 54.42639474917956  median: 47.0  max: 117  min: 0  count_non_na: 8532  count: 118\n",
      "HANDSET_TYPE_ix  mean: 5.319034224097515  median: 6.0  max: 6  min: 0  count_non_na: 8532  count: 7\n"
     ]
    }
   ],
   "source": [
    "## summarize features:\n",
    "for col in all_features1:\n",
    "    print(col,\" mean:\", data_train[col].mean(), \" median:\",data_train[col].median(), \" max:\",data_train[col].max(), \n",
    "          \" min:\",data_train[col].min(), \" count_non_na:\",data_train[col].count(),\n",
    "         #\" cat:\",pd.Categorical(data_train[col]).dtype.name,\n",
    "         \" count:\",len(pd.Series.unique(data_train[col])))\n",
    "    #data_train[col] = data_train[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### create new dataset with dropped NA values, and dataset with NAs filled by zeros (medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8532 2803\n",
      "data_train shape is: (8532, 56)  train_na_dropped shape is: (8384, 56)\n"
     ]
    }
   ],
   "source": [
    "features1 = [i for i in all_features1 if i not in ['IS_RETIREE',]]\n",
    "features2 = [i for i in all_features2 if i not in ['IS_RETIREE',]]\n",
    "print(len(data_train[features1]), len(data_train[features1].dropna()))\n",
    "data_train['SUPPORT_3G'] = data_train['SUPPORT_3G'].fillna(0)\n",
    "\n",
    "train_na_dropped = data_train.dropna()\n",
    "data_train_with_na = copy.deepcopy(data_train)\n",
    "\n",
    "for f in data_train.columns.values:\n",
    "    data_train[f] = data_train[f].fillna(0)\n",
    "rng = sklearn.utils.check_random_state(0)\n",
    "print(\"data_train shape is:\", data_train.shape, \" train_na_dropped shape is:\", train_na_dropped.shape)\n",
    "#x_train, y_train = shuffle(data_train[features], data_train['IS_RETIREE'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initial check of all classifiers for different number of k best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without dummies, NAs filled with 0:\n",
      "\n",
      "Number of best features chosen: 25\n",
      "Model scores\n",
      "ada_boost         0.741363\n",
      "bagging-forest    0.711883\n",
      "bagging-ridge     0.725037\n",
      "bagging-tree      0.643377\n",
      "extra_tree        0.652038\n",
      "forest            0.643786\n",
      "gaussian          0.695699\n",
      "grad_boost        0.749647\n",
      "passive           0.604232\n",
      "ridge             0.724796\n",
      "sgd               0.614815\n",
      "svc               0.602776\n",
      "tree              0.548438\n",
      "xgboost           0.754451\n",
      "dtype: float64\n",
      "Number of best features chosen: 35\n",
      "Model scores\n",
      "ada_boost         0.741043\n",
      "bagging-forest    0.717574\n",
      "bagging-ridge     0.724555\n",
      "bagging-tree      0.656849\n",
      "extra_tree        0.641960\n",
      "forest            0.656977\n",
      "gaussian          0.684960\n",
      "grad_boost        0.749444\n",
      "passive           0.613302\n",
      "ridge             0.724591\n",
      "sgd               0.615896\n",
      "svc               0.630484\n",
      "tree              0.550261\n",
      "xgboost           0.751782\n",
      "dtype: float64\n",
      "Number of best features chosen: 38\n",
      "Model scores\n",
      "ada_boost         0.736994\n",
      "bagging-forest    0.712559\n",
      "bagging-ridge     0.726377\n",
      "bagging-tree      0.660896\n",
      "extra_tree        0.648217\n",
      "forest            0.658265\n",
      "gaussian          0.682760\n",
      "grad_boost        0.750299\n",
      "passive           0.619049\n",
      "ridge             0.725819\n",
      "sgd               0.608214\n",
      "svc               0.610555\n",
      "tree              0.551085\n",
      "xgboost           0.753535\n",
      "dtype: float64\n",
      "Number of best features chosen: 40\n",
      "Model scores\n",
      "ada_boost         0.742972\n",
      "bagging-forest    0.722034\n",
      "bagging-ridge     0.724620\n",
      "bagging-tree      0.655253\n",
      "extra_tree        0.664407\n",
      "forest            0.663131\n",
      "gaussian          0.677855\n",
      "grad_boost        0.752265\n",
      "passive           0.593105\n",
      "ridge             0.723437\n",
      "sgd               0.618314\n",
      "svc               0.608789\n",
      "tree              0.545835\n",
      "xgboost           0.754399\n",
      "dtype: float64\n",
      "Without dummies, NAs dropped:\n",
      "\n",
      "Number of best features chosen: 25\n",
      "Model scores\n",
      "ada_boost         0.738116\n",
      "bagging-forest    0.722866\n",
      "bagging-ridge     0.727357\n",
      "bagging-tree      0.656252\n",
      "extra_tree        0.655841\n",
      "forest            0.660542\n",
      "gaussian          0.698262\n",
      "grad_boost        0.748454\n",
      "passive           0.610093\n",
      "ridge             0.727659\n",
      "sgd               0.610796\n",
      "svc               0.592388\n",
      "tree              0.544277\n",
      "xgboost           0.751940\n",
      "dtype: float64\n",
      "Number of best features chosen: 35\n",
      "Model scores\n",
      "ada_boost         0.739539\n",
      "bagging-forest    0.718430\n",
      "bagging-ridge     0.727854\n",
      "bagging-tree      0.651072\n",
      "extra_tree        0.645264\n",
      "forest            0.645912\n",
      "gaussian          0.686328\n",
      "grad_boost        0.752794\n",
      "passive           0.624708\n",
      "ridge             0.728118\n",
      "sgd               0.621898\n",
      "svc               0.614943\n",
      "tree              0.540062\n",
      "xgboost           0.754177\n",
      "dtype: float64\n",
      "Number of best features chosen: 38\n",
      "Model scores\n",
      "ada_boost         0.744450\n",
      "bagging-forest    0.725552\n",
      "bagging-ridge     0.726316\n",
      "bagging-tree      0.645194\n",
      "extra_tree        0.651835\n",
      "forest            0.644484\n",
      "gaussian          0.687883\n",
      "grad_boost        0.754542\n",
      "passive           0.596167\n",
      "ridge             0.726628\n",
      "sgd               0.613010\n",
      "svc               0.607371\n",
      "tree              0.547760\n",
      "xgboost           0.758431\n",
      "dtype: float64\n",
      "Number of best features chosen: 40\n",
      "Model scores\n",
      "ada_boost         0.742493\n",
      "bagging-forest    0.718939\n",
      "bagging-ridge     0.727224\n",
      "bagging-tree      0.656528\n",
      "extra_tree        0.651188\n",
      "forest            0.665295\n",
      "gaussian          0.682805\n",
      "grad_boost        0.756227\n",
      "passive           0.623097\n",
      "ridge             0.726379\n",
      "sgd               0.608663\n",
      "svc               0.625330\n",
      "tree              0.554878\n",
      "xgboost           0.754411\n",
      "dtype: float64\n",
      "With dummies, NAs filled with 0:\n",
      "\n",
      "Number of best features chosen: 35\n",
      "Model scores\n",
      "ada_boost         0.734143\n",
      "bagging-forest    0.711331\n",
      "bagging-ridge     0.726298\n",
      "bagging-tree      0.653647\n",
      "extra_tree        0.658967\n",
      "forest            0.655521\n",
      "gaussian          0.678049\n",
      "grad_boost        0.749792\n",
      "passive           0.626921\n",
      "ridge             0.725735\n",
      "sgd               0.626951\n",
      "svc               0.614703\n",
      "tree              0.545075\n",
      "xgboost           0.751937\n",
      "dtype: float64\n",
      "Number of best features chosen: 45\n",
      "Model scores\n",
      "ada_boost         0.740199\n",
      "bagging-forest    0.724604\n",
      "bagging-ridge     0.725760\n",
      "bagging-tree      0.651138\n",
      "extra_tree        0.655307\n",
      "forest            0.653688\n",
      "gaussian          0.637611\n",
      "grad_boost        0.751115\n",
      "passive           0.611544\n",
      "ridge             0.725605\n",
      "sgd               0.619356\n",
      "svc               0.614607\n",
      "tree              0.543829\n",
      "xgboost           0.754883\n",
      "dtype: float64\n",
      "Number of best features chosen: 48\n",
      "Model scores\n",
      "ada_boost         0.742082\n",
      "bagging-forest    0.725873\n",
      "bagging-ridge     0.727304\n",
      "bagging-tree      0.661160\n",
      "extra_tree        0.657514\n",
      "forest            0.659888\n",
      "gaussian          0.630165\n",
      "grad_boost        0.750206\n",
      "passive           0.614094\n",
      "ridge             0.727805\n",
      "sgd               0.627555\n",
      "svc               0.610617\n",
      "tree              0.544892\n",
      "xgboost           0.756179\n",
      "dtype: float64\n",
      "Number of best features chosen: 50\n",
      "Model scores\n",
      "ada_boost         0.743186\n",
      "bagging-forest    0.715253\n",
      "bagging-ridge     0.724711\n",
      "bagging-tree      0.647108\n",
      "extra_tree        0.652027\n",
      "forest            0.652166\n",
      "gaussian          0.629266\n",
      "grad_boost        0.752861\n",
      "passive           0.605138\n",
      "ridge             0.726022\n",
      "sgd               0.627150\n",
      "svc               0.613664\n",
      "tree              0.551557\n",
      "xgboost           0.752291\n",
      "dtype: float64\n",
      "With dummies, NAs dropped:\n",
      "\n",
      "Number of best features chosen: 35\n",
      "Model scores\n",
      "ada_boost         0.738629\n",
      "bagging-forest    0.725180\n",
      "bagging-ridge     0.727832\n",
      "bagging-tree      0.648561\n",
      "extra_tree        0.672601\n",
      "forest            0.655053\n",
      "gaussian          0.682282\n",
      "grad_boost        0.753036\n",
      "passive           0.637962\n",
      "ridge             0.729000\n",
      "sgd               0.628135\n",
      "svc               0.595056\n",
      "tree              0.548240\n",
      "xgboost           0.755491\n",
      "dtype: float64\n",
      "Number of best features chosen: 45\n",
      "Model scores\n",
      "ada_boost         0.741235\n",
      "bagging-forest    0.728184\n",
      "bagging-ridge     0.726282\n",
      "bagging-tree      0.643129\n",
      "extra_tree        0.647994\n",
      "forest            0.652832\n",
      "gaussian          0.659465\n",
      "grad_boost        0.753621\n",
      "passive           0.626319\n",
      "ridge             0.728258\n",
      "sgd               0.630548\n",
      "svc               0.626776\n",
      "tree              0.552245\n",
      "xgboost           0.755003\n",
      "dtype: float64\n",
      "Number of best features chosen: 48\n",
      "Model scores\n",
      "ada_boost         0.739542\n",
      "bagging-forest    0.721609\n",
      "bagging-ridge     0.728542\n",
      "bagging-tree      0.653819\n",
      "extra_tree        0.671114\n",
      "forest            0.660719\n",
      "gaussian          0.637716\n",
      "grad_boost        0.751162\n",
      "passive           0.628050\n",
      "ridge             0.728072\n",
      "sgd               0.616535\n",
      "svc               0.613373\n",
      "tree              0.552883\n",
      "xgboost           0.753527\n",
      "dtype: float64\n",
      "Number of best features chosen: 50\n",
      "Model scores\n",
      "ada_boost         0.737833\n",
      "bagging-forest    0.719255\n",
      "bagging-ridge     0.728602\n",
      "bagging-tree      0.662383\n",
      "extra_tree        0.673465\n",
      "forest            0.660392\n",
      "gaussian          0.634953\n",
      "grad_boost        0.754886\n",
      "passive           0.622921\n",
      "ridge             0.729458\n",
      "sgd               0.626281\n",
      "svc               0.643267\n",
      "tree              0.541441\n",
      "xgboost           0.755324\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def choose_k_best(k,dataset,features):\n",
    "    selection = SelectKBest(k=k)\n",
    "    return SelectKBest(k=k).fit_transform(dataset[features],dataset['IS_RETIREE'])\n",
    "\n",
    "def init_Check(features, dataset, k_list):  \n",
    "    def score_model(model):\n",
    "        return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n",
    "    for k in k_list:\n",
    "        data_kbest = choose_k_best(k, dataset, features)   \n",
    "        X, y = shuffle(data_kbest, dataset['IS_RETIREE'], random_state=0)\n",
    "        \n",
    "        skf = cv.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "        score_metric = 'roc_auc'\n",
    "        scores = {}\n",
    "        \n",
    "        scores['svc'] = score_model(SVC())\n",
    "        scores['tree'] = score_model(DecisionTreeClassifier()) \n",
    "        scores['extra_tree'] = score_model(ExtraTreesClassifier())\n",
    "        scores['forest'] = score_model(RandomForestClassifier())\n",
    "        scores['ada_boost'] = score_model(AdaBoostClassifier())\n",
    "        scores['bagging-tree'] = score_model(BaggingClassifier())\n",
    "        scores['bagging-forest'] = score_model(BaggingClassifier(base_estimator = RandomForestClassifier()))\n",
    "        scores['bagging-ridge'] = score_model(BaggingClassifier(base_estimator = RidgeClassifier()))\n",
    "        scores['grad_boost'] = score_model(GradientBoostingClassifier())\n",
    "        scores['ridge'] = score_model(RidgeClassifier())\n",
    "        scores['passive'] = score_model(PassiveAggressiveClassifier())\n",
    "        scores['sgd'] = score_model(SGDClassifier())\n",
    "        scores['gaussian'] = score_model(GaussianNB())\n",
    "        scores['xgboost'] = score_model(XGBClassifier())\n",
    "\n",
    "        # Print the scores\n",
    "        model_scores = pd.DataFrame(scores).mean()\n",
    "        model_scores.sort_values(ascending=False)\n",
    "        model_scores.to_csv('model_scores.csv', index=False)\n",
    "        print(\"Number of best features chosen:\",k)\n",
    "        print('Model scores\\n{}'.format(model_scores))\n",
    "\n",
    "print(\"\\nWithout dummies, NAs filled with 0:\")\n",
    "k_list = [25, 35, 38, len(features1)]\n",
    "init_Check(features1, data_train, k_list)\n",
    "print(\"\\nWithout dummies, NAs dropped:\")\n",
    "init_Check(features1, train_na_dropped, k_list)\n",
    "\n",
    "print(\"\\nWith dummies, NAs filled with 0:\")\n",
    "k_list = [35, 45, 48, len(features2)]\n",
    "init_Check(features2, data_train, k_list)\n",
    "print(\"\\nWith dummies, NAs dropped:\")\n",
    "init_Check(features2, train_na_dropped, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bagging classificator with built in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_bagging(k, x_data, y_data, base_estimator, rng, params):\n",
    "    n = len(x_data)\n",
    "    AUC_list = list()\n",
    "    for i in range(k):\n",
    "        start = int((n*i)/k)\n",
    "        end = int((n*(i+1))/k-1)\n",
    "        x_valid = x_data[start:end+1]\n",
    "        y_valid = y_data[start:end+1] \n",
    "        x_train = x_data[:start].append(x_data[end+1:])\n",
    "        y_train = y_data[:start].append(y_data[end+1:])\n",
    "        \n",
    "        predictions = BaggingClassifier(base_estimator = base_estimator, n_estimators = 40,\n",
    "                        random_state = rng,\n",
    "                        **params).fit(x_train, y_train).predict_proba(x_valid)\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_valid, predictions[:,1], pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        AUC_list.append(auc)\n",
    "    return (np.mean(AUC_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: False   max_samples: 0.5   max_features: 0.5   AUC: 0.741495039464   estimator: FOREST_35    bootstrap_features: False\n",
      "bootstrap: False   max_samples: 0.75   max_features: 0.5   AUC: 0.744104279611   estimator: FOREST_35    bootstrap_features: False\n",
      "bootstrap: False   max_samples: 0.5   max_features: 0.5   AUC: 0.500716294807   estimator: RIDGE    bootstrap_features: False\n",
      "bootstrap: False   max_samples: 0.75   max_features: 0.5   AUC: 0.500782125858   estimator: RIDGE    bootstrap_features: False\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = shuffle(data_train[features2], data_train['IS_RETIREE'], random_state=0)\n",
    "\n",
    "grid = ParameterGrid({\"max_samples\" : [0.5, 0.75],\n",
    "                      \"max_features\": [0.5,],\n",
    "                      \"bootstrap\": [False,],\n",
    "                      \"bootstrap_features\": [False,] })\n",
    "base_estimators = [\n",
    "    RandomForestClassifier(n_estimators=35),\n",
    "    RidgeClassifier(),\n",
    "                   \n",
    "                  ]\n",
    "est_names = ['FOREST_35',\n",
    "             'RIDGE',\n",
    "            ]\n",
    "i = 0\n",
    "for base_estimator in base_estimators:\n",
    "    for params in grid:\n",
    "        auc = k_fold_cross_validation_bagging(10, x_train, y_train, base_estimator, rng, params)\n",
    "        print(\"bootstrap:\",params['bootstrap'], \"  max_samples:\", params['max_samples'],\"  max_features:\", params['max_features'], \n",
    "              \"  AUC:\",auc, \"  estimator:\",est_names[i],\"   bootstrap_features:\",params[\"bootstrap_features\"])\n",
    "    i += 1    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PCA and K-means datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19872057  0.13157198  0.11285504  0.0985253   0.09003118  0.05720297\n",
      "  0.03867009  0.03659307  0.02976893  0.02387298  0.02071065  0.01862513\n",
      "  0.01826621  0.01600806  0.01173828  0.01124685  0.01012342  0.0097324\n",
      "  0.00896957  0.00832734  0.00772957  0.00703826  0.00633228  0.00469435\n",
      "  0.00442665  0.0037168   0.00257133  0.00224181  0.00212544  0.00153674]\n",
      "0.993973264886\n",
      "(8532, 25)\n"
     ]
    }
   ],
   "source": [
    "train_st = StandardScaler().fit_transform(data_train[features1])\n",
    "x_train, y_train = shuffle(train_st, data_train['IS_RETIREE'], random_state=rng)\n",
    "pca = PCA(n_components=30)\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "# Use combined features to transform dataset:\n",
    "cf = combined_features.fit(x_train, y_train)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "X_features = cf.transform(x_train)\n",
    "\n",
    "train_pca = pd.DataFrame(X_features)\n",
    "#print(train_pca[:5])\n",
    "def create_data_kmeans(k, data_train, features):\n",
    "    data_kmeans = (KMeans(n_clusters = k)).fit(data_train[features],data_train['IS_RETIREE'])\n",
    "    data_kmeans = (data_kmeans.transform(data_train[features],data_train['IS_RETIREE']))\n",
    "    print(data_kmeans.shape)\n",
    "    return data_kmeans\n",
    "X_features = create_data_kmeans(25, data_train, features1) \n",
    "train_kmeans = pd.DataFrame(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8532, 15) (8532, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_kmeans.shape, train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## XGBoost with built in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_xgboost(k, x_data, y_data, params, features):\n",
    "    n = len(x_data)\n",
    "    AUC_list = list()\n",
    "    #imp = np.array([0]*len(features))\n",
    "    for i in range(k):\n",
    "        start = int((n*i)/k)\n",
    "        end = int((n*(i+1))/k-1)\n",
    "        x_valid = x_data[start:end+1]\n",
    "        y_valid = y_data[start:end+1] \n",
    "        x_train = x_data[:start].append(x_data[end+1:])\n",
    "        y_train = y_data[:start].append(y_data[end+1:])\n",
    "        \n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        x_fit, x_eval, y_fit, y_eval= train_test_split(x_train, y_train, test_size=0.2)\n",
    "        clf = clf.fit(x_fit, y_fit, \n",
    "                              early_stopping_rounds=30, \n",
    "                              eval_metric=\"auc\", \n",
    "                eval_set=[(x_eval, y_eval)])\n",
    "        predictions = clf.predict_proba(x_valid)\n",
    "        #imp = imp + np.array(clf.feature_importances_)                  \n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_valid, predictions[:,1], pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        AUC_list.append(auc)\n",
    "    #print(sorted(dict(zip(features,imp)).items(), key = lambda x: x[1]))    \n",
    "    return (np.mean(AUC_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n_est: 400  subsample: 0.8  nthread: 5  colsample_bytree: 0.85  max_depth: 5  AUC: 0.751344714123\n",
      " n_est: 400  subsample: 0.8  nthread: 5  colsample_bytree: 0.85  max_depth: 5  AUC: 0.659279856614\n",
      " n_est: 400  subsample: 0.8  nthread: 5  colsample_bytree: 0.85  max_depth: 5  AUC: 0.505545359945\n"
     ]
    }
   ],
   "source": [
    "grid_xgb = ParameterGrid({\n",
    "                      \"gamma\":[0,],\n",
    "                      \"missing\" : [np.nan,],\n",
    "                      \"max_depth\": [5,],\n",
    "                      \"n_estimators\": [400,],\n",
    "                      \"learning_rate\": [0.03,],\n",
    "                      \"nthread\":[5,],\n",
    "                      \"subsample\":[0.8,],\n",
    "                      \"colsample_bytree\":[0.85,],\n",
    "                      \"seed\":[4242,]})\n",
    "\n",
    "## initial dataset with NAs:\n",
    "x_train, y_train = shuffle(data_train_with_na[features1], data_train_with_na['IS_RETIREE'], random_state=rng)\n",
    "for params in grid_xgb:\n",
    "    print(\" n_est:\",params['n_estimators'],\" subsample:\",params['subsample'],\" nthread:\",params['nthread'],\n",
    "        \" colsample_bytree:\",params['colsample_bytree'], \" max_depth:\",params['max_depth'],\n",
    "        \" AUC:\",k_fold_cross_validation_xgboost(10, x_train, y_train, params, features1))\n",
    "\n",
    "## distances to clusters(kmeans):\n",
    "x_train, y_train = shuffle(train_kmeans, data_train['IS_RETIREE'], random_state=rng)\n",
    "for params in grid_xgb:\n",
    "    print(\" n_est:\",params['n_estimators'],\" subsample:\",params['subsample'],\" nthread:\",params['nthread'],\n",
    "        \" colsample_bytree:\",params['colsample_bytree'], \" max_depth:\",params['max_depth'],\n",
    "        \" AUC:\",k_fold_cross_validation_xgboost(10, x_train, y_train, params, features1))\n",
    "\n",
    "## PCA dataset:    \n",
    "x_train, y_train = shuffle(train_pca, data_train['IS_RETIREE'], random_state=rng)\n",
    "for params in grid_xgb:\n",
    "    print(\" n_est:\",params['n_estimators'],\" subsample:\",params['subsample'],\" nthread:\",params['nthread'],\n",
    "        \" colsample_bytree:\",params['colsample_bytree'], \" max_depth:\",params['max_depth'],\n",
    "        \" AUC:\",k_fold_cross_validation_xgboost(10, x_train, y_train, params, features1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## check for other classifier with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_class(k, classifier, x_data, y_data, features):\n",
    "    n = len(x_data)\n",
    "    AUC_list = list()\n",
    "    imp = np.array([0]*len(features))\n",
    "    for i in range(k):\n",
    "        start = int((n*i)/k)\n",
    "        end = int((n*(i+1))/k-1)\n",
    "        x_valid = x_data[start:end+1]\n",
    "        y_valid = y_data[start:end+1] \n",
    "        x_train = x_data[:start].append(x_data[end+1:])\n",
    "        y_train = y_data[:start].append(y_data[end+1:])\n",
    "        \n",
    "        predictions = classifier.fit(x_train, y_train).predict_proba(x_valid)\n",
    "        imp = imp + np.array(classifier.feature_importances_)\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_valid, predictions[:,1], pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        AUC_list.append(auc)\n",
    "    print(sorted(dict(zip(features,imp)).items(), key = lambda x: x[1])[:5])\n",
    "    return np.mean(AUC_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OUT_ROUM_MIN_3M', 0.059629811231823746), ('OUT_ROUM_CNT_3M', 0.061066518814388709), ('IN_ROUM_CNT_3M', 0.064297190797614992), ('DATA_CNT_SCH_MORN_6M', 0.070204144191630916), ('SMS_CNT_EV_6M', 0.070684634117065687)]\n",
      "GrBoost_AUC: 0.752517522121  learning_rate: 0.1  n_est: 360  max_leaf_nodes 4  min_sample_split: 5  max_features 1 \n",
      "\n",
      "\n",
      "[('DATA_CNT_EV_6M', 0.15587122673199488), ('V_OUT_INT_DUR_3M', nan), ('SMS_CNT_SCH_MORN_6M', nan), ('SUPPORT_3G', 0.094039448280466148), ('DATA_CNT_NGHT_6M', 0.11843377715254405)]\n",
      "AdaBoost_AUC: 0.721991795242  learning_rate: 0.1  n_est: 300 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = shuffle(data_train[features1], data_train['IS_RETIREE'], random_state=rng)\n",
    "#x_train, y_train = shuffle(x_train_pca, train['IS_RETIREE'], random_state=0)\n",
    "\n",
    "grid1 = ParameterGrid({\"n_estimators\" : [360, ],\n",
    "                      \"learning_rate\": [0.1,],\n",
    "                      \"subsample\": [1,],\n",
    "                      \"max_leaf_nodes\": [4,] ,\n",
    "                     \"min_samples_split\":[5,],\n",
    "                     \"max_features\": [1,],\n",
    "                     \"max_depth\": [None,],\n",
    "                     \"random_state\":[0,]}) \n",
    "for params in grid1:\n",
    "    classifier = GradientBoostingClassifier(**params)\n",
    "    print(\"GrBoost_AUC:\",k_fold_cross_validation_class(10,classifier, x_train, y_train, features1),\n",
    "         \" learning_rate:\",params['learning_rate'],\" n_est:\",params['n_estimators'],\n",
    "         \" max_leaf_nodes\",params['max_leaf_nodes'], \" min_sample_split:\",params['min_samples_split'],\n",
    "         \" max_features\",params['max_features'],'\\n\\n') \n",
    "\n",
    "grid2 = ParameterGrid({\"n_estimators\" : [300,],\"learning_rate\": [0.1,],\"random_state\":[0,]}) \n",
    "params0 = grid1[0]\n",
    "base_estimator = GradientBoostingClassifier(**params0)\n",
    "for params in grid2:\n",
    "    classifier = AdaBoostClassifier(base_estimator = base_estimator,**params)\n",
    "    print(\"AdaBoost_AUC:\",k_fold_cross_validation_class(10,classifier, x_train, y_train, features1),\n",
    "         \" learning_rate:\",params['learning_rate'],\" n_est:\",params['n_estimators'],'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 4 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "['OUT_ROUM_MIN_3M', 'OUT_ROUM_CNT_3M', 'IN_ROUM_CNT_3M']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "x_train, y_train = shuffle(data_train[features1], data_train['IS_RETIREE'], random_state=rng)\n",
    "estimators = [XGBClassifier()]\n",
    "names = ['ada','ridge','grad','xgb']\n",
    "for estimator in estimators:\n",
    "    print(names[estimators.index(estimator)])\n",
    "    #X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "    selector = RFECV(estimator, step=1, cv=10, scoring = 'roc_auc')\n",
    "    selector = selector.fit(x_train, y_train)\n",
    "    print(selector.ranking_)\n",
    "\n",
    "    features_fin = list(np.array(features1)[selector.support_])\n",
    "    print([f for f in features1 if f not in features_fin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(data_train[features1], data_train['IS_RETIREE'], random_state=rng)\n",
    "selector = RFECV(XGBClassifier(), step=1, cv=10, scoring = 'roc_auc')\n",
    "selector = selector.fit(x_train, y_train)\n",
    "print(selector.ranking_)\n",
    "\n",
    "features_fin = list(np.array(features1)[selector.support_])\n",
    "print([f for f in features1 if f not in features_fin])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### search for best parameters for XGBClassifier and GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97244536,  0.02755464],\n",
       "       [ 0.93554454,  0.06445546],\n",
       "       [ 0.94251143,  0.05748857],\n",
       "       ..., \n",
       "       [ 0.91301853,  0.08698147],\n",
       "       [ 0.97319598,  0.02680402],\n",
       "       [ 0.93252713,  0.06747287]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_AUC: 0.756995713177  Parameters: {'nthread': 9, 'max_depth': 7, 'subsample': 0.95, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.9, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 360, 'gamma': 0}\n",
      "Best_AUC: 0.758911513787  Parameters: {'nthread': 9, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 0.9, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 360, 'gamma': 0}\n",
      "Best_AUC: 0.758077893252  Parameters: {'nthread': 9, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 360, 'gamma': 0}\n",
      "Best_AUC: 0.759613835785  Parameters: {'nthread': 9, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 460, 'gamma': 0}\n",
      "Best_AUC: 0.759613835785  Parameters: {'nthread': 9, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 460, 'gamma': 0}\n",
      "Best_AUC: 0.759613835785  Parameters: {'nthread': 30, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 460, 'gamma': 0}\n",
      "Best_AUC: 0.759613835785  Parameters: {'nthread': 30, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 460, 'gamma': 0}\n",
      "Best_AUC: 0.760638729213  Parameters: {'nthread': 30, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 460, 'gamma': 0.8}\n",
      "Best_AUC: 0.760638729213  Parameters: {'nthread': 30, 'max_depth': 7, 'subsample': 1, 'colsample_bytree': 0.3, 'colsample_bylevel': 1, 'learning_rate': 0.01, 'min_child_weight': 1, 'n_estimators': 460, 'gamma': 0.8}\n"
     ]
    }
   ],
   "source": [
    "#for col in [i for i in features if i not in ['IS_RETIREE','SUPPORT_3G']]:\n",
    "#    data_train[col] = data_train[col].fillna(0)\n",
    "#data_kmeans = choose_k_best(38, data_train, features)   \n",
    "X, y = shuffle(data_train_with_na[features1], data_train_with_na['IS_RETIREE'], random_state=rng)\n",
    "\n",
    "skf = cv.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "score_metric = 'roc_auc'\n",
    "scores = {}\n",
    "\n",
    "def score_model(model):\n",
    "    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n",
    "\n",
    "dict_xgb = {\n",
    "        \"gamma\":[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "        \"missing\" : [np.nan,],\n",
    "        \"max_depth\": [10,9,8,7,6,5,4,3,],\n",
    "        \"colsample_bylevel\":[1,0.9,0.8,0.7,0.6,0.5,],\n",
    "        \"n_estimators\": [460, 410, 360, 310, 260],\n",
    "        \"learning_rate\": [0.5,0.45,0.4,0.3,0.2,0.1,0.05,0.01,0.001],\n",
    "        \"nthread\":[30,20,18,16,14,12,10,8,6,5,4,3],\n",
    "        \"subsample\":[1,0.9, 0.85, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3],\n",
    "        \"colsample_bytree\":[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,],\n",
    "        \"min_child_weight\":[1,0.9,0.8,0.7,0.6,0.5],\n",
    "        \"seed\":[40,41,50,100,2022,4441]}\n",
    "\n",
    "params_list = ['max_depth','subsample','colsample_bylevel',\n",
    "               'n_estimators','learning_rate','nthread','colsample_bytree','gamma','min_child_weight']\n",
    "cur_params = dict(zip(params_list, [2, 0.95, 0.9, 360, 0.01, 9, 0.3, 0, 1]))\n",
    "best_params = dict(zip(params_list, [2, 0.95, 0.9, 360, 0.01, 9, 0.3, 0, 1]))\n",
    "\n",
    "for param in params_list:\n",
    "    best_AUC = 0\n",
    "    cur_params = copy.deepcopy(best_params)\n",
    "    for param_val in dict_xgb[param]:\n",
    "        cur_params[param] = param_val\n",
    "        AUC = score_model(XGBClassifier(**cur_params)).mean()\n",
    "        if AUC > best_AUC + 0.001:\n",
    "            best_params[param] = param_val\n",
    "            best_AUC = AUC\n",
    "            \n",
    "    print( \"Best_AUC:\",best_AUC,\n",
    "        \" Parameters:\",best_params\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8532, 56)\n",
      "Best_AUC: 0.751393527764  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 1, 'max_features': 2, 'learning_rate': 0.01, 'n_estimators': 500, 'max_leaf_nodes': 5}\n",
      "Best_AUC: 0.751393527764  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 1, 'max_features': 2, 'learning_rate': 0.01, 'n_estimators': 500, 'max_leaf_nodes': 5}\n",
      "Best_AUC: 0.758033363674  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 1, 'max_features': 2, 'learning_rate': 0.05, 'n_estimators': 500, 'max_leaf_nodes': 5}\n",
      "Best_AUC: 0.759379861638  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 0.8, 'max_features': 2, 'learning_rate': 0.05, 'n_estimators': 500, 'max_leaf_nodes': 5}\n",
      "Best_AUC: 0.759379861638  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 0.8, 'max_features': 2, 'learning_rate': 0.05, 'n_estimators': 500, 'max_leaf_nodes': 5}\n",
      "Best_AUC: 0.759379861638  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 0.8, 'max_features': 2, 'learning_rate': 0.05, 'n_estimators': 500, 'max_leaf_nodes': 5}\n",
      "Best_AUC: 0.759379861638  Parameters: {'min_samples_split': 7, 'max_depth': 3, 'subsample': 0.8, 'max_features': 2, 'learning_rate': 0.05, 'n_estimators': 500, 'max_leaf_nodes': 5}\n"
     ]
    }
   ],
   "source": [
    "train = data_train.dropna()\n",
    "print(train.shape)\n",
    "\n",
    "X, y = shuffle(train_na_dropped[features1], train_na_dropped['IS_RETIREE'], random_state=rng)\n",
    "skf = cv.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "score_metric = 'roc_auc'\n",
    "scores = {}\n",
    "\n",
    "def score_model(model):\n",
    "    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n",
    "\n",
    "dict_gradboost = {\"n_estimators\" : [260,360,460,560],\n",
    "                      \"learning_rate\": [0.1,0.05,0.2,0.3,0.15,0.35,0.01,0.45,0.5,0.55],\n",
    "                      \"subsample\": [0.9,0.8,0.7,0.6,0.5,],\n",
    "                      \"max_leaf_nodes\": [8,7,6,5,4,3],\n",
    "                     \"min_samples_split\":[8,7,6,5,4,3],\n",
    "                     \"max_features\": [7,6,2,4,3,5],\n",
    "                     \"max_depth\": [1,2,3],\n",
    "                     \"random_state\":[0,100]}\n",
    "\n",
    "params_list = ['max_depth','n_estimators','learning_rate','subsample','max_leaf_nodes','min_samples_split','max_features']\n",
    "cur_params = dict(zip(params_list, [2, 500, 0.01, 1, 5, 7, 2]))\n",
    "best_params2 = dict(zip(params_list, [2, 500, 0.01, 1, 5, 7, 2]))\n",
    "best_AUC = 0\n",
    "\n",
    "for param in params_list:\n",
    "    cur_params = copy.deepcopy(best_params2)\n",
    "    for param_val in dict_gradboost[param]:\n",
    "        cur_params[param] = param_val\n",
    "        AUC = score_model(GradientBoostingClassifier(**cur_params)).mean()\n",
    "        if AUC > best_AUC + 0.001:\n",
    "            best_params2[param] = param_val\n",
    "            best_AUC = AUC\n",
    "            \n",
    "            \n",
    "    print( \"Best_AUC:\",best_AUC,\n",
    "        \" Parameters:\",best_params2\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without dummies, NAs filled with 0:\n",
      "Number of best features chosen: 12\n",
      "Model scores average\n",
      "grad_boost    0.726958\n",
      "xgb           0.715972\n",
      "dtype: float64\n",
      "Number of best features chosen: 13\n",
      "Model scores average\n",
      "grad_boost    0.728080\n",
      "xgb           0.714606\n",
      "dtype: float64\n",
      "Number of best features chosen: 14\n",
      "Model scores average\n",
      "grad_boost    0.741615\n",
      "xgb           0.733443\n",
      "dtype: float64\n",
      "Number of best features chosen: 15\n",
      "Model scores average\n",
      "grad_boost    0.739566\n",
      "xgb           0.735990\n",
      "dtype: float64\n",
      "Number of best features chosen: 16\n",
      "Model scores average\n",
      "grad_boost    0.741532\n",
      "xgb           0.735403\n",
      "dtype: float64\n",
      "Number of best features chosen: 17\n",
      "Model scores average\n",
      "grad_boost    0.741249\n",
      "xgb           0.739804\n",
      "dtype: float64\n",
      "Number of best features chosen: 18\n",
      "Model scores average\n",
      "grad_boost    0.740369\n",
      "xgb           0.736807\n",
      "dtype: float64\n",
      "Number of best features chosen: 19\n",
      "Model scores average\n",
      "grad_boost    0.741497\n",
      "xgb           0.732624\n",
      "dtype: float64\n",
      "Number of best features chosen: 20\n",
      "Model scores average\n",
      "grad_boost    0.744369\n",
      "xgb           0.742824\n",
      "dtype: float64\n",
      "Number of best features chosen: 21\n",
      "Model scores average\n",
      "grad_boost    0.745705\n",
      "xgb           0.745194\n",
      "dtype: float64\n",
      "Number of best features chosen: 22\n",
      "Model scores average\n",
      "grad_boost    0.749772\n",
      "xgb           0.745743\n",
      "dtype: float64\n",
      "Number of best features chosen: 23\n",
      "Model scores average\n",
      "grad_boost    0.756274\n",
      "xgb           0.752746\n",
      "dtype: float64\n",
      "Number of best features chosen: 24\n",
      "Model scores average\n",
      "grad_boost    0.753697\n",
      "xgb           0.750585\n",
      "dtype: float64\n",
      "Number of best features chosen: 25\n",
      "Model scores average\n",
      "grad_boost    0.755034\n",
      "xgb           0.752122\n",
      "dtype: float64\n",
      "Number of best features chosen: 26\n",
      "Model scores average\n",
      "grad_boost    0.754258\n",
      "xgb           0.750636\n",
      "dtype: float64\n",
      "Number of best features chosen: 27\n",
      "Model scores average\n",
      "grad_boost    0.756388\n",
      "xgb           0.755483\n",
      "dtype: float64\n",
      "Number of best features chosen: 28\n",
      "Model scores average\n",
      "grad_boost    0.757631\n",
      "xgb           0.753890\n",
      "dtype: float64\n",
      "Number of best features chosen: 29\n",
      "Model scores average\n",
      "grad_boost    0.758681\n",
      "xgb           0.753223\n",
      "dtype: float64\n",
      "Number of best features chosen: 30\n",
      "Model scores average\n",
      "grad_boost    0.757850\n",
      "xgb           0.756297\n",
      "dtype: float64\n",
      "Number of best features chosen: 31\n",
      "Model scores average\n",
      "grad_boost    0.753894\n",
      "xgb           0.754036\n",
      "dtype: float64\n",
      "Number of best features chosen: 32\n",
      "Model scores average\n",
      "grad_boost    0.760038\n",
      "xgb           0.754121\n",
      "dtype: float64\n",
      "Number of best features chosen: 33\n",
      "Model scores average\n",
      "grad_boost    0.756952\n",
      "xgb           0.753845\n",
      "dtype: float64\n",
      "Number of best features chosen: 34\n",
      "Model scores average\n",
      "grad_boost    0.762149\n",
      "xgb           0.758021\n",
      "dtype: float64\n",
      "Number of best features chosen: 35\n",
      "Model scores average\n",
      "grad_boost    0.759829\n",
      "xgb           0.757559\n",
      "dtype: float64\n",
      "Number of best features chosen: 36\n",
      "Model scores average\n",
      "grad_boost    0.757377\n",
      "xgb           0.755115\n",
      "dtype: float64\n",
      "Number of best features chosen: 37\n",
      "Model scores average\n",
      "grad_boost    0.758723\n",
      "xgb           0.754112\n",
      "dtype: float64\n",
      "Number of best features chosen: 38\n",
      "Model scores average\n",
      "grad_boost    0.760772\n",
      "xgb           0.758900\n",
      "dtype: float64\n",
      "Number of best features chosen: 39\n",
      "Model scores average\n",
      "grad_boost    0.758722\n",
      "xgb           0.757734\n",
      "dtype: float64\n",
      "Number of best features chosen: 40\n",
      "Model scores average\n",
      "grad_boost    0.761027\n",
      "xgb           0.759720\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def choose_k_best(k,dataset,features):\n",
    "    selection = SelectKBest(k=k)\n",
    "    return SelectKBest(k=k).fit_transform(dataset[features],dataset['IS_RETIREE'])\n",
    "\n",
    "def fin_Check_GradBoost(features, dataset, k_list):  \n",
    "    def score_model(model):\n",
    "        return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n",
    "    AUC_grad = list()\n",
    "    AUC_xgb = list()\n",
    "    for k in k_list:\n",
    "        data_kbest = choose_k_best(k, dataset, features)   \n",
    "        X, y = shuffle(data_kbest, dataset['IS_RETIREE'], random_state=rng)\n",
    "        \n",
    "        skf = cv.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "        score_metric = 'roc_auc'\n",
    "        scores = {}\n",
    "        \n",
    "        scores['grad_boost'] = score_model(GradientBoostingClassifier(**best_params2))\n",
    "        scores['xgb'] = score_model(XGBClassifier(**best_params))\n",
    "        # Print the scores\n",
    "        model_scores = pd.DataFrame(scores).mean()\n",
    "        print(\"Number of best features chosen:\",k)\n",
    "        print('Model scores average\\n{}'.format(model_scores))\n",
    "        AUC_grad.append(model_scores['grad_boost'])\n",
    "        AUC_xgb.append(model_scores['xgb'])\n",
    "    return (AUC_grad, AUC_xgb)   \n",
    "print(\"\\nWithout dummies, NAs filled with 0:\")\n",
    "k_list = list(range(12, len(features1)+1))\n",
    "AUC_grad, AUC_xgb = fin_Check_GradBoost(features1, data_train, k_list)\n",
    "plt.plot(k_list, AUC_grad)\n",
    "plt.plot(k_list, AUC_xgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load test dataset and prepare for predictions\n",
    "## (tranformation is the same as with training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(r'A:\\KAGGLE\\Kaggle_Kyivstar\\Xtest.csv', sep = ',')\n",
    "\n",
    "category = list(set(data_test['HANDSET_VENDOR_CVAL']))\n",
    "category\n",
    "data_test['HANDSET_VENDOR_CVAL_ix'] = [category.index(i) for i in data_test['HANDSET_VENDOR_CVAL']]\n",
    "category = list(set(data_test['HANDSET_TYPE']))\n",
    "category\n",
    "data_test['HANDSET_TYPE_ix'] = [category.index(i) for i in data_test['HANDSET_TYPE']]\n",
    "data_train['AVG_CHARGE_3M_GROUP'] = data_train['AVG_CHARGE_3M_GROUP']-1\n",
    "data_test['SUPPORT_3G'] = data_test['SUPPORT_3G'].fillna(0)\n",
    "data_test = pd.concat([data_test, pd.get_dummies(data_test['AVG_CHARGE_3M_GROUP'],prefix='gr')], axis=1)\n",
    "data_test = pd.concat([data_test, pd.get_dummies(data_test['HANDSET_TYPE_ix'],prefix='handset')], axis=1)\n",
    "for feature in features1:\n",
    "    data_test[feature] = data_test[feature].fillna(0)\n",
    "#print(data_test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.00309129282327 0.996908707177\n",
      "[   0 3524]\n",
      "[3524    0]\n"
     ]
    }
   ],
   "source": [
    "## Make predictions:\n",
    "\n",
    "x_train, y_train = shuffle(data_train[features_fin], data_train['IS_RETIREE'], random_state=0)\n",
    "params_list_xgb = ['max_depth','subsample','colsample_bylevel',\n",
    "               'n_estimators','learning_rate','nthread','colsample_bytree','gamma','min_child_weight']\n",
    "params_xgb = dict(zip(params_list_xgb, [2, 1, 0.9, 400, 0.01, 9, 0.5, 0, 1]))\n",
    "fin_model = GradientBoostingClassifier(**best_params).fit(x_train, y_train)\n",
    "#print(list(zip(features_fin, fin_model.feature_importances_)))\n",
    "predictions = fin_model.predict_proba(data_test[features_fin])\n",
    "data_test['IS_RETIREE'] = predictions[:,1]\n",
    "print(predictions.mean(),predictions.min(),predictions.max())\n",
    "print(sum(predictions<0.1))\n",
    "print(sum(predictions>0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "data_test.to_csv('A:\\KAGGLE\\Kaggle_Kyivstar\\Prediction.csv', \n",
    "                                              index= False, columns = ['SUBS_ID', 'IS_RETIREE'],quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
